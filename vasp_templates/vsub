#!/bin/bash
#
# 6.4.3_gpu vsub marvin 10/05/2024
#
export inpfile=${1:? no input file name given -- exit}
export jobname=${inpfile%%.inp}
export catname=$jobname.$$
export outfile=$catname.sum
export Programm=/opt/software/ag_mctc_bredow/vasp.6.4.3_gpu/bin/vasp_gam

if [ ! -x "$Programm" ]; then echo "\n $Programm is not available\n"; exit; fi

cat > $jobname.job <<ENDE
#!/bin/bash
#SBATCH --partition=intelsr_short
#SBATCH --time=0-8:00:00
##SBATCH --gpus-per-node=8
#SBATCH --ntasks-per-node=64
##SBATCH --cpus-per-gpu=16
#SBATCH --nodes=1
#SBATCH --job-name=$jobname

tdir=$(ws_allocate $catname 10)
echo "creating \$tdir"
echo "node list: " \$SLURM_JOB_NODELIST
echo "nodes: " \$SLURM_NNODES
echo "jobid: " \$SLURM_JOBID
echo "ntasks: " \$SLURM_NTASKS
echo "gpus: " \$SLURM_GPUS
echo "tasks per node: " \$SLURM_TASKS_PER_NODE
echo "nprocs: " \$SLURM_NPROCS
echo "cpus on node: " \$SLURM_CPUS_ON_NODE
echo "gpus on node: " \$SLURM_GPUS_ON_NODE

module purge
module load ASE
module load imkl/2023.2.0

export PATH=/opt/software/ag_mctc_bredow/nvidia_new/Linux_x86_64/23.11/compilers/bin:/opt/software/ag_mctc_bredow/nvidia_new/Linux_x86_64/23.11/comm_libs/openmpi4/bin:\$PATH

export LD_LIBRARY_PATH=/opt/software/ag_mctc_bredow/nvidia_new/Linux_x86_64/23.11/comm_libs/nvshmem/lib:\
/opt/software/ag_mctc_bredow/nvidia_new/Linux_x86_64/23.11/comm_libs/nccl/lib:\
/opt/software/ag_mctc_bredow/nvidia_new/Linux_x86_64/23.11/math_libs/lib64:\
/opt/software/ag_mctc_bredow/nvidia_new/Linux_x86_64/23.11/compilers/lib:\
/opt/software/ag_mctc_bredow/nvidia_new/Linux_x86_64/23.11/compilers/extras/qd/lib:\
/opt/software/ag_mctc_bredow/nvidia_new/Linux_x86_64/23.11/cuda/extras/CUPTI/lib64:\
/opt/software/ag_mctc_bredow/nvidia_new/Linux_x86_64/23.11/cuda/lib64:\
/opt/software/ag_mctc_bredow/nvidia_new/Linux_x86_64/23.11/comm_libs/12.3/openmpi4/openmpi-4.1.5/lib:\
\$LD_LIBRARY_PATH

export CPATH=/opt/software/ag_mctc_bredow/nvidia_new/Linux_x86_64/23.11/compilers/extras/qd/include/qd:\
/opt/software/ag_mctc_bredow/nvidia_new/Linux_x86_64/23.11/comm_libs/nvshmem/include:\
/opt/software/ag_mctc_bredow/nvidia_new/Linux_x86_64/23.11/comm_libs/nccl/include:\
/opt/software/ag_mctc_bredow/nvidia_new/Linux_x86_64/23.11/math_libs/include:\
/opt/software/ag_mctc_bredow/nvidia_new/Linux_x86_64/23.11/comm_libs/12.3/openmpi4/openmpi-4.1.5/include:\
\$CPATH

ulimit -s unlimited

echo -e "$PWD/${BASH_SOURCE#./}\nstarted from \$SLURM_SUBMIT_HOST,\nrunning \$(which mpirun) $Programm\non \$(hostname)\nin \$tdir"

cd \$SLURM_SUBMIT_DIR


if [ -s $inpfile ];    then cp $inpfile    \$tdir/POSCAR ; else echo "VASP input file $inpfile is empty or does not exist!"; exit; fi
if [ -s INCAR ]; then cp INCAR \$tdir/; else echo " INCAR is empty or does not exist!"; exit; fi
if [ -s KPOINTS ]; then cp KPOINTS \$tdir/; else echo "KPOINTS is empty or does not exist!"; exit; fi
if [ -s POTCAR ]; then cp POTCAR \$tdir/; else echo "POTCAR is empty or does not exist!"; exit; fi

cd \$tdir


date
cat INCAR
#ldd /opt/software/ag_mctc_bredow/vasp.6.4.3_gpu/bin/vasp_gam
mpirun $Programm
# python3 $jobname.py
date

cp OUTCAR \$SLURM_SUBMIT_DIR/$jobname.$$.pbed3
if [-s CONTCAR ]; then cp CONTCAR \$SLURM_SUBMIT_DIR/$jobname.$$.inp ; fi
cp vasprun.xml \$SLURM_SUBMIT_DIR/$jobname.$$.xml
cp CONTCAR \$SLURM_SUBMIT_DIR/$jobname.$$.inp
cp CHGCAR \$SLURM_SUBMIT_DIR/$jobname.$$.chgcar
cp PARCHG \$SLURM_SUBMIT_DIR/$jobname.$$.parchg


ENDE
sbatch --export=NONE -o $outfile $jobname.job
rm $jobname.job
